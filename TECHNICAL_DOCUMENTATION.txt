================================================================================
PHARMACEUTICAL DISPOSAL PREDICTION SYSTEM
Technical Documentation for Professionals
================================================================================

EXECUTIVE SUMMARY
-----------------

This system implements a multi-class, multi-label machine learning pipeline for
predicting pharmaceutical disposal information from generic medicine names. The
solution employs a hybrid approach combining sentence embeddings, feature engineering,
two-stage hierarchical classification, and similarity-based retrieval to achieve
high accuracy across diverse prediction tasks.

Key Technical Highlights:
- Sentence-BERT embeddings (384 dimensions) + engineered features (35 dimensions)
- Two-stage hierarchical classification for high-cardinality outputs
- Multi-label classification for disposal methods
- OCR integration with image preprocessing pipeline
- RESTful API with FastAPI framework
- Production-ready error handling and validation

SYSTEM ARCHITECTURE
-------------------

ARCHITECTURAL OVERVIEW
----------------------

The system follows a modular, pipeline-based architecture:

Input Layer
    ↓
[Text Input] OR [Image Input → OCR → Text Extraction]
    ↓
Preprocessing & Validation Layer
    ↓
Feature Extraction Layer
    ├── Sentence Embeddings (Sentence-BERT)
    └── Engineered Features (35 features)
    ↓
Model Inference Layer
    ├── Dosage Form (Two-Stage Classifier)
    ├── Manufacturer (Two-Stage Classifier)
    ├── Disposal Category (Single-Stage Classifier)
    ├── Method of Disposal (Multi-Label Classifier)
    └── Text Retrieval (Similarity Search)
    ↓
Post-Processing & Analysis Generation
    ↓
Output Layer (JSON/Text/Summary)

COMPONENT ARCHITECTURE
----------------------

1. TRAINING PIPELINE (train.py)
   - Data Loading & Preprocessing
   - Text Normalization & Standardization
   - Feature Engineering
   - Model Training (XGBoost/RandomForest)
   - Model Serialization
   - Similarity Index Construction

2. PREDICTION PIPELINE (predict.py)
   - Input Detection & Routing
   - OCR Processing (if image)
   - Feature Extraction
   - Model Inference
   - Result Aggregation
   - Analysis Generation

3. API LAYER (api.py)
   - FastAPI Application
   - Request Validation (Pydantic)
   - Error Handling
   - Response Formatting
   - CORS Middleware

4. SUPPORTING MODULES
   - input_handler.py: Input validation & routing
   - ocr_extractor.py: OCR with preprocessing
   - analysis_generator.py: Text generation
   - test.py: Model evaluation & testing

DATA PIPELINE
-------------

DATA SOURCE
-----------
- Dataset: rwanda_fda_medicines_with_disposal.csv
- Records: ~2,338
- Features: Generic Name, Dosage Form, Manufacturer, Disposal Category,
           Method of Disposal, Handling Method, Disposal Remarks

PREPROCESSING STEPS
------------------

1. TEXT NORMALIZATION
   - Lowercase conversion
   - Whitespace normalization
   - Special character handling
   - Case-insensitive matching

2. DOSAGE FORM STANDARDIZATION
   - Maps variations to canonical forms
   - Example: "tablet" → "tablets", "film-coated tablet" → "film coated tablets"
   - Reduces 441 unique forms to ~200 standardized forms

3. RARE CLASS MERGING
   - Classes with <3 occurrences → "OTHER"
   - Prevents overfitting on rare classes
   - Improves generalization

4. FEATURE ENGINEERING
   - Length features (name length, word count, compound count)
   - Keyword features (20 pharmaceutical terms)
   - Numeric pattern extraction (dosage strengths)
   - Character frequency (spaces, commas, hyphens)
   - Prefix/suffix patterns (anti-, co-, pre-, pro-)
   - Total: 35 engineered features

FEATURE EXTRACTION
------------------

1. SENTENCE EMBEDDINGS
   - Model: all-MiniLM-L6-v2 (Sentence-BERT)
   - Dimensions: 384
   - Purpose: Semantic similarity capture
   - Implementation: sentence-transformers library

2. ENGINEERED FEATURES
   - Length Features (3): name_length, word_count, compound_count
   - Keyword Features (20): tablet, capsule, injection, suspension, etc.
   - Numeric Features (3): num_numbers, max_number, min_number
   - Character Features (4): space_count, comma_count, slash_count, hyphen_count
   - Prefix Features (5): anti-, co-, de-, pre-, pro-
   - Total: 35 features

3. COMBINED FEATURE VECTOR
   - Embeddings (384) + Engineered (35) = 419 dimensions
   - Used for: Dosage Form, Manufacturer, Disposal Category, Method of Disposal
   - Embeddings only (384): Used for similarity search

MODEL ARCHITECTURE
------------------

TWO-STAGE HIERARCHICAL CLASSIFICATION
-------------------------------------

Purpose: Improve accuracy on high-cardinality classification tasks
         (Dosage Form: 441 classes, Manufacturer: 898 classes)

Architecture:
    Stage 1: Broad Category Classification
        Input: Combined features (419 dims)
        Output: Broad category (7-8 categories)
        Model: XGBoost/RandomForest
        
    Stage 2: Specific Class Classification
        Input: Combined features (419 dims)
        Output: Specific class within category
        Model: XGBoost/RandomForest (one per category)

Broad Categories:
- Dosage Form: tablet_category, capsule_category, injection_category,
              liquid_category, powder_category, topical_category,
              suppository_category, other_category
              
- Manufacturer: laboratory_category, pharma_category, limited_category,
                sas_category, other_category

Advantages:
- Reduces search space in Stage 2
- Improves confidence scores (60-87% vs 8-29% single-stage)
- Better handling of class imbalance
- More interpretable predictions

SINGLE-STAGE CLASSIFICATION
---------------------------

Disposal Category:
- Classes: 7
- Model: XGBoost/RandomForest
- Features: Combined (419 dims)
- Accuracy: ~95%+

MULTI-LABEL CLASSIFICATION
--------------------------

Method of Disposal:
- Labels: Variable (typically 4-6 per medicine)
- Model: MultiOutputClassifier with XGBoost/RandomForest base
- Features: Combined (419 dims)
- Encoding: MultiLabelBinarizer
- Output: Binary predictions per label with confidence scores

SIMILARITY-BASED RETRIEVAL
--------------------------

Handling Method & Disposal Remarks:
- Method: NearestNeighbors (cosine distance)
- Features: Sentence embeddings only (384 dims)
- K: 1 (retrieves most similar)
- Purpose: Template-based text retrieval
- Accuracy: ~90%+ (based on similarity threshold)

MODEL TRAINING DETAILS
---------------------

HYPERPARAMETERS
---------------

XGBoost (if available):
- n_estimators: 100-200
- max_depth: 8-10
- learning_rate: 0.1
- subsample: 0.8
- colsample_bytree: 0.8
- min_child_weight: 3
- gamma: 0.1
- eval_metric: mlogloss/logloss
- random_state: 42

RandomForest (fallback):
- n_estimators: 100-200
- max_depth: 15-20
- min_samples_split: 5
- min_samples_leaf: 2
- class_weight: 'balanced'
- random_state: 42

TRAINING PROCEDURE
------------------

1. Data Loading & Preprocessing
2. Embedding Generation (batch processing)
3. Feature Engineering
4. Feature Combination
5. Model Training:
   a. Two-stage models (Dosage Form, Manufacturer)
   b. Single-stage model (Disposal Category)
   c. Multi-label model (Method of Disposal)
6. Similarity Index Construction
7. Model Serialization (pickle format)

PERFORMANCE METRICS
-------------------

Training Accuracy:
- Disposal Category: ~95%+ (7 classes)
- Dosage Form: ~70-80% (441 classes, reduced to ~200)
- Manufacturer: ~60-70% (898 classes, reduced to ~400)
- Method of Disposal: ~85%+ (multi-label F1)
- Text Retrieval: ~90%+ (cosine similarity >0.8)

Confidence Scores:
- Two-stage approach: 60-87% (vs 8-29% single-stage)
- Disposal Category: 90-98%
- Method of Disposal: 75-98% per label

INFERENCE PIPELINE
------------------

PREDICTION FLOW
---------------

1. Input Reception
   - Text: Direct validation
   - Image: OCR extraction → validation

2. Text Normalization
   - Lowercase, whitespace cleanup
   - OCR error correction

3. Feature Extraction
   - Generate embeddings (384 dims)
   - Extract engineered features (35 dims)
   - Combine to 419 dims

4. Model Inference
   - Dosage Form: Stage 1 → Stage 2 → Top-K
   - Manufacturer: Stage 1 → Stage 2 → Top-K
   - Disposal Category: Direct prediction
   - Method of Disposal: Multi-label prediction
   - Text Retrieval: Nearest neighbor search

5. Post-Processing
   - Confidence score calculation
   - Result formatting
   - Analysis generation

OCR INTEGRATION
---------------

LIBRARY: EasyOCR (primary), Tesseract (fallback)

PREPROCESSING PIPELINE:
1. Image Loading (OpenCV)
2. Grayscale Conversion
3. Resizing (if <300px)
4. Denoising (fastNlMeansDenoising)
5. Contrast Enhancement (CLAHE)
6. Sharpening (kernel filter)
7. Adaptive Thresholding
8. Deskewing (rotation correction)

TEXT EXTRACTION:
- EasyOCR: readtext() with confidence threshold >0.5
- Tesseract: Multiple PSM modes (6, 11, 7)
- Character whitelisting for medicine names

POST-PROCESSING:
- OCR error correction (common misreadings)
- Medicine name identification (pattern matching)
- Validation (length, character checks)

ACCURACY: ~85-90% on clear medicine package images

API DESIGN
----------

FRAMEWORK: FastAPI (ASGI)

ENDPOINTS:
----------

1. GET /health
   - Health check
   - Model availability verification
   - Returns: HealthResponse

2. POST /predict/text
   - Text-based prediction
   - Request: TextPredictionRequest
   - Returns: PredictionResponse

3. POST /predict/image
   - Image-based prediction
   - Request: Multipart form data
   - Returns: PredictionResponse

4. POST /predict
   - Flexible endpoint (text or image)
   - Request: Multipart form data
   - Returns: PredictionResponse

REQUEST/RESPONSE MODELS:
------------------------

TextPredictionRequest:
- medicine_name: str (3-100 chars)
- output_format: Optional[str] ("full"|"summary"|"json")

PredictionResponse:
- success: bool
- medicine_name: Optional[str]
- input_type: Optional[str]
- predictions: Dict[str, Any]
- analysis: Optional[str]
- messages: List[str]
- errors: List[str]

ERROR HANDLING:
--------------
- 200: Success
- 400: Bad Request (invalid input)
- 500: Internal Server Error
- 503: Service Unavailable (models not loaded)

CORS: Enabled for all origins (configurable for production)

MIDDLEWARE:
-----------
- CORS Middleware
- Exception Handlers (404, 500)
- Request Validation (Pydantic)

CODE STRUCTURE
--------------

MODULE ORGANIZATION
-------------------

predict.py (Main Entry Point)
├── predict_from_input(): Orchestrates entire pipeline
├── display_result(): CLI output formatting
└── main(): Command-line interface

train.py (Model Training)
├── load_and_preprocess_data(): Data loading
├── standardize_dosage_form(): Standardization
├── merge_rare_classes(): Class merging
├── extract_features(): Feature engineering
├── create_embeddings(): Embedding generation
├── train_two_stage_model(): Two-stage training
├── train_categorical_model(): Single-stage training
├── train_multilabel_model(): Multi-label training
├── create_similarity_index(): Similarity search setup
└── save_models(): Model serialization

test.py (Model Testing)
├── load_models(): Model loading
├── predict_all(): Inference pipeline
├── display_predictions(): Output formatting
├── interactive_mode(): CLI testing
└── batch_test_mode(): Batch evaluation

api.py (API Server)
├── TextPredictionRequest: Pydantic model
├── PredictionResponse: Pydantic model
├── health_check(): Health endpoint
├── predict_from_text(): Text prediction endpoint
├── predict_from_image(): Image prediction endpoint
└── predict_flexible(): Flexible endpoint

input_handler.py (Input Processing)
├── detect_input_type(): Input type detection
├── process_input(): Input processing
├── validate_medicine_name(): Validation
└── normalize_medicine_name(): Normalization

ocr_extractor.py (OCR Processing)
├── MedicineNameExtractor: Main class
├── preprocess_image(): Image preprocessing
├── extract_all_text(): Text extraction
├── identify_medicine_name(): Name identification
└── correct_ocr_errors(): Error correction

analysis_generator.py (Output Generation)
├── generate_analysis(): Full analysis
├── generate_short_summary(): Summary
└── generate_json_analysis(): JSON output

DEPENDENCIES
------------

CORE:
- Python 3.7+
- pandas >= 1.5.0
- numpy >= 1.23.0, < 2.0.0
- scikit-learn >= 1.2.0

ML/AI:
- sentence-transformers >= 2.2.0
- xgboost >= 1.7.0 (optional)

OCR:
- easyocr >= 1.7.0
- opencv-python >= 4.8.0
- Pillow >= 10.0.0

API:
- fastapi >= 0.104.0
- uvicorn[standard] >= 0.24.0
- python-multipart >= 0.0.6

COMPATIBILITY:
- pickle5 >= 0.0.11 (Python < 3.8)

DEPLOYMENT CONSIDERATIONS
-------------------------

PRODUCTION READINESS
--------------------

1. MODEL SERVING
   - Models loaded at startup (memory resident)
   - Lazy loading for OCR (first use)
   - Model versioning recommended

2. SCALABILITY
   - Stateless API design (horizontal scaling)
   - Consider model caching (Redis)
   - Batch prediction endpoint for efficiency

3. MONITORING
   - Logging: Request/response logging
   - Metrics: Prediction latency, accuracy tracking
   - Alerts: Model degradation, error rates

4. SECURITY
   - Input validation (Pydantic)
   - File upload limits
   - CORS configuration (restrict origins)
   - Rate limiting (prevent abuse)

5. ERROR HANDLING
   - Graceful degradation (fallback models)
   - User-friendly error messages
   - Error logging and tracking

OPTIMIZATION OPPORTUNITIES
---------------------------

1. MODEL OPTIMIZATION
   - Model quantization (reduce size)
   - ONNX conversion (faster inference)
   - Batch prediction optimization

2. FEATURE OPTIMIZATION
   - Feature selection (remove redundant)
   - Dimensionality reduction (PCA)
   - Embedding fine-tuning

3. INFERENCE OPTIMIZATION
   - Model caching (frequent predictions)
   - Async processing (non-blocking)
   - GPU acceleration (if available)

4. API OPTIMIZATION
   - Response compression
   - Caching (Redis/Memcached)
   - Connection pooling

TECHNICAL CHALLENGES & SOLUTIONS
---------------------------------

CHALLENGE 1: High Cardinality Classification
---------------------------------------------
Problem: 441 dosage forms, 898 manufacturers → low confidence scores

Solution: Two-stage hierarchical classification
- Stage 1: Predict broad category (7-8 classes)
- Stage 2: Predict specific class within category
- Result: Confidence scores improved from 8-29% to 60-87%

CHALLENGE 2: Class Imbalance
----------------------------
Problem: Rare classes (<3 occurrences) cause overfitting

Solution: Rare class merging
- Classes with <3 occurrences → "OTHER"
- Prevents overfitting
- Improves generalization

CHALLENGE 3: Text Variations
----------------------------
Problem: Same dosage form written differently
        ("tablet" vs "tablets" vs "film-coated tablet")

Solution: Standardization mapping
- Maps variations to canonical forms
- Reduces 441 forms to ~200 standardized forms
- Improves model consistency

CHALLENGE 4: OCR Accuracy
-------------------------
Problem: OCR errors in medicine name extraction

Solution: Multi-stage preprocessing + error correction
- Image preprocessing (denoising, contrast, deskewing)
- Multiple OCR attempts (original + processed)
- OCR error correction dictionary
- Pattern-based medicine name identification

CHALLENGE 5: NumPy 2.0 Compatibility
----------------------------------
Problem: NumPy 2.0 breaks compatibility with some dependencies

Solution: Version constraint + fix script
- requirements.txt: numpy < 2.0.0
- fix_numpy.py: Automated downgrade script
- Clear error messages with solution

CHALLENGE 6: Text Retrieval Quality
------------------------------------
Problem: Handling Method & Disposal Remarks need accurate retrieval

Solution: Similarity search with embeddings
- Uses sentence embeddings (semantic similarity)
- Cosine distance for nearest neighbor
- Retrieves from most similar medicine
- ~90%+ accuracy on similar medicines

FUTURE IMPROVEMENTS
-------------------

SHORT-TERM (1-3 months)
-----------------------
1. Model Fine-tuning
   - Hyperparameter optimization (Optuna)
   - Cross-validation for better metrics
   - Ensemble methods

2. Feature Enhancement
   - Additional engineered features
   - Embedding fine-tuning on domain data
   - External data integration (drug databases)

3. API Enhancements
   - Batch prediction endpoint
   - WebSocket support for streaming
   - GraphQL API option

4. Monitoring & Logging
   - Structured logging (JSON)
   - Metrics dashboard (Prometheus/Grafana)
   - Error tracking (Sentry)

MEDIUM-TERM (3-6 months)
-------------------------
1. Model Improvements
   - Deep learning models (BERT fine-tuning)
   - Transfer learning from larger datasets
   - Active learning for rare classes

2. Multi-language Support
   - OCR for multiple languages
   - Translation integration
   - International medicine databases

3. Mobile Integration
   - Mobile app SDK
   - Offline model support
   - Camera integration

4. Real-time Updates
   - Model retraining pipeline
   - A/B testing framework
   - Continuous learning

LONG-TERM (6-12 months)
-----------------------
1. Advanced Features
   - Drug interaction checking
   - Expiry date detection
   - Batch number validation
   - Regulatory compliance checking

2. Scalability
   - Distributed model serving
   - Cloud deployment (AWS/GCP/Azure)
   - Auto-scaling infrastructure

3. Integration
   - Pharmacy management systems
   - Hospital information systems
   - Government regulatory systems

4. Research
   - Explainable AI (SHAP values)
   - Model interpretability
   - Fairness analysis

TESTING STRATEGY
----------------

UNIT TESTS
----------
- Feature extraction functions
- Text normalization
- OCR preprocessing
- Model inference logic

INTEGRATION TESTS
-----------------
- End-to-end prediction pipeline
- API endpoint testing
- OCR → Prediction flow
- Error handling scenarios

PERFORMANCE TESTS
-----------------
- Latency benchmarks
- Throughput testing
- Memory usage profiling
- Model loading time

ACCURACY TESTS
--------------
- Test set evaluation
- Cross-validation
- Confidence score calibration
- Error analysis

CODE QUALITY
------------

STYLE: PEP 8 compliant (with exceptions for readability)

DOCUMENTATION:
- Docstrings for all functions
- Type hints where applicable
- README files for usage
- API documentation (Swagger/ReDoc)

ERROR HANDLING:
- Try-except blocks for critical operations
- User-friendly error messages
- Logging for debugging
- Graceful degradation

VALIDATION:
- Input validation (Pydantic models)
- Type checking
- Range validation
- Format validation

CONCLUSION
----------

This system demonstrates a production-ready ML pipeline with:
- Robust architecture (modular, scalable)
- High accuracy on key tasks (95%+ disposal category)
- Multiple input methods (text, image)
- Professional API design (RESTful, documented)
- Comprehensive error handling
- Extensible design for future enhancements

The two-stage hierarchical approach successfully addresses high-cardinality
classification challenges, while the hybrid feature approach (embeddings +
engineered features) provides both semantic understanding and domain-specific
patterns.

The system is ready for deployment with appropriate monitoring, scaling, and
security measures in place.

================================================================================
END OF TECHNICAL DOCUMENTATION
================================================================================

